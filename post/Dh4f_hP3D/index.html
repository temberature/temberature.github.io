<html>
  <head>
    <meta charset="utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title>é¦–æ¬¡ä¸GPT-4V(ision)çš„äº’åŠ¨ä½“éªŒ First Impressions with GPT-4V(ision)   | ç¬¬ä¸‰å¤§è„‘</title>
<link rel="shortcut icon" href="https://temberature.github.io/favicon.ico?v=1707130463873">
<link href="https://cdn.jsdelivr.net/npm/remixicon@2.3.0/fonts/remixicon.css" rel="stylesheet">
<link rel="stylesheet" href="https://temberature.github.io/styles/main.css">
<link rel="alternate" type="application/atom+xml" title="é¦–æ¬¡ä¸GPT-4V(ision)çš„äº’åŠ¨ä½“éªŒ First Impressions with GPT-4V(ision)   | ç¬¬ä¸‰å¤§è„‘ - Atom Feed" href="https://temberature.github.io/atom.xml">
<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Droid+Serif:400,700">


<script async src="https://www.googletagmanager.com/gtag/js?id=G-WY2N173F2W"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-WY2N173F2W');
</script>


    <meta name="description" content="
James Gallagher, Piotr Skalski
è©¹å§†æ–¯Â·åŠ æ‹‰æ ¼å°”ï¼Œçš®å¥¥ç‰¹Â·æ–¯å¡å°”æ–¯åŸº
Sep 27, 2023 2023å¹´9æœˆ27æ—¥
9 min read

On September 25th, 2023, OpenAI a..." />
    <meta name="keywords" content="" />
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.10.0/katex.min.css">
    <script src="//cdn.jsdelivr.net/gh/highlightjs/cdn-release@11.5.1/build/highlight.min.js"></script>
  </head>
  <body>
    <div class="main">
      <div class="main-content">
        <div class="site-header">
  <a href="https://temberature.github.io">
  <img class="avatar" src="https://temberature.github.io/images/avatar.png?v=1707130463873" alt="">
  </a>
  <h1 class="site-title">
    ç¬¬ä¸‰å¤§è„‘
  </h1>
  <p class="site-description">
    æ™ºèƒ½å…±ç”Ÿâ€”â€”ä¸‡ç‰©äº’è”ï¼Œè£‚éš™æœ‰å…‰ã€‚
  </p>
  <div class="menu-container">
    
      
        <a href="/" class="menu">
          é¦–é¡µ
        </a>
      
    
      
        <a href="/archives" class="menu">
          å½’æ¡£
        </a>
      
    
      
        <a href="/tags" class="menu">
          æ ‡ç­¾
        </a>
      
    
      
        <a href="/post/about" class="menu">
          å…³äº
        </a>
      
    
      
        <a href="https://temberature.github.io/post/w3wJXnHGy" class="menu">
          å‹é“¾
        </a>
      
    
      
        <a href="https://temberature.github.io/tag/99sjMFOuu/" class="menu">
          è½¬å½•
        </a>
      
    
  </div>
  <div class="social-container">
    
      
        <a href="https://github.com/temberature" target="_blank">
          <i class="ri-github-line"></i>
        </a>
      
    
      
        <a href="https://twitter.com/TianDatong" target="_blank">
          <i class="ri-twitter-line"></i>
        </a>
      
    
      
    
      
    
      
    
  </div>
</div>

        <div class="post-detail">
          <article class="post">
            <h2 class="post-title">
              é¦–æ¬¡ä¸GPT-4V(ision)çš„äº’åŠ¨ä½“éªŒ First Impressions with GPT-4V(ision)  
            </h2>
            <div class="post-info">
              <span>
                2023-09-28
              </span>
              <span>
                23 min read
              </span>
              
            </div>
            
            <div class="post-content-wrapper">
              <div class="post-content" v-pre>
                <p><a href="https://blog.roboflow.com/author/james/"></a></p>
<p><a href="https://blog.roboflow.com/author/james/"></a><a href="https://blog.roboflow.com/author/james/">James Gallagher</a>, <a href="https://blog.roboflow.com/author/skalskip/">Piotr Skalski</a><br>
è©¹å§†æ–¯Â·åŠ æ‹‰æ ¼å°”ï¼Œçš®å¥¥ç‰¹Â·æ–¯å¡å°”æ–¯åŸº</p>
<p>Sep 27, 2023 2023å¹´9æœˆ27æ—¥</p>
<p>9 min read</p>
<figure data-type="image" tabindex="1"><img src="/content/images/size/w1000/2023/09/First-Impressions-with-GPT-4V-ision--Capabilities2.jpg" alt="" loading="lazy"></figure>
<p>On September 25th, 2023, <a href="https://help.openai.com/en/articles/6825453-chatgpt-release-notes?ref=blog.roboflow.com">OpenAI announced the rollout of two new features</a> that extend how people can interact with its recent and most advanced model, <a href="https://openai.com/research/gpt-4?ref=blog.roboflow.com">GPT-4</a>: the ability to ask questions about images and to use speech as an input to a query.<br>
2023å¹´9æœˆ25æ—¥ï¼ŒOpenAIå®£å¸ƒæ¨å‡ºä¸¤é¡¹æ–°åŠŸèƒ½ï¼Œæ‰©å±•äº†äººä»¬ä¸å…¶æœ€æ–°å’Œæœ€å…ˆè¿›çš„æ¨¡å‹GPT-4çš„äº’åŠ¨æ–¹å¼ï¼šå³å¯ä»¥å¯¹å›¾åƒæé—®ï¼Œä¹Ÿå¯ä»¥å°†è¯­éŸ³ä½œä¸ºæŸ¥è¯¢çš„è¾“å…¥ã€‚</p>
<p>This functionality marks GPT-4â€™s move into being a <a href="https://blog.roboflow.com/multimodal-models/">multimodal model</a>. This means that the model can accept multiple â€œmodalitiesâ€ of input â€“ text and images â€“ and return results based on those inputs. Bing Chat, developed by Microsoft in partnership with OpenAI, and Googleâ€™s Bard model both support images as input, too. <a href="https://blog.roboflow.com/using-google-bard-with-images/">Read our comparison post to see how Bard and Bing perform with image inputs</a>.<br>
è¿™ä¸ªåŠŸèƒ½æ ‡å¿—ç€GPT-4è¿›å…¥äº†å¤šæ¨¡æ€æ¨¡å‹çš„é¢†åŸŸã€‚è¿™æ„å‘³ç€è¯¥æ¨¡å‹å¯ä»¥æ¥å—å¤šç§è¾“å…¥æ–¹å¼ï¼ŒåŒ…æ‹¬æ–‡æœ¬å’Œå›¾åƒï¼Œå¹¶æ ¹æ®è¿™äº›è¾“å…¥è¿”å›ç»“æœã€‚å¾®è½¯ä¸OpenAIåˆä½œå¼€å‘çš„Bing Chatä»¥åŠè°·æ­Œçš„Bardæ¨¡å‹ä¹Ÿæ”¯æŒå›¾åƒä½œä¸ºè¾“å…¥ã€‚é˜…è¯»æˆ‘ä»¬çš„æ¯”è¾ƒæ–‡ç« ï¼Œäº†è§£Bardå’ŒBingåœ¨å›¾åƒè¾“å…¥æ–¹é¢çš„è¡¨ç°ã€‚</p>
<p>In this guide, we are going to share our first impressions with the GPT-4V image input feature. We will run through a series of experiments to test the functionality of GPT-4V, showing where the model performs well and where it struggles.<br>
åœ¨æœ¬æŒ‡å—ä¸­ï¼Œæˆ‘ä»¬å°†åˆ†äº«æˆ‘ä»¬å¯¹GPT-4Vå›¾åƒè¾“å…¥åŠŸèƒ½çš„ç¬¬ä¸€å°è±¡ã€‚æˆ‘ä»¬å°†è¿›è¡Œä¸€ç³»åˆ—å®éªŒï¼Œæµ‹è¯•GPT-4Vçš„åŠŸèƒ½ï¼Œå±•ç¤ºæ¨¡å‹åœ¨å“ªäº›æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä»¥åŠåœ¨å“ªäº›æ–¹é¢é‡åˆ°å›°éš¾ã€‚</p>
<p><em>Note: This article shows a limited series of tests our team performed; your results will vary depending on the questions you ask and the images you use in a prompt. Tag us on social media @roboflow with your findings using GPT-4V. We would love to see more tests using the model!<br>
æ³¨æ„ï¼šæœ¬æ–‡å±•ç¤ºäº†æˆ‘ä»¬å›¢é˜Ÿè¿›è¡Œçš„ä¸€ç³»åˆ—æœ‰é™æµ‹è¯•ï¼›æ‚¨çš„ç»“æœå°†å–å†³äºæ‚¨æå‡ºçš„é—®é¢˜å’Œåœ¨æç¤ºä¸­ä½¿ç”¨çš„å›¾åƒã€‚åœ¨ç¤¾äº¤åª’ä½“ä¸Šæ ‡è®°æˆ‘ä»¬çš„è´¦å·@roboflowï¼Œå¹¶ä½¿ç”¨GPT-4Våˆ†äº«æ‚¨çš„å‘ç°ã€‚æˆ‘ä»¬å¾ˆä¹æ„çœ‹åˆ°æ›´å¤šä½¿ç”¨è¯¥æ¨¡å‹çš„æµ‹è¯•ï¼</em></p>
<p>Without further ado, letâ€™s get started!<br>
åºŸè¯ä¸å¤šè¯´ï¼Œæˆ‘ä»¬å¼€å§‹å§ï¼</p>
<h2 id="what-is-gpt-4v-gpt-4væ˜¯ä»€ä¹ˆ">What is GPT-4V? GPT-4Væ˜¯ä»€ä¹ˆï¼Ÿ</h2>
<p><a href="https://cdn.openai.com/papers/GPTV_System_Card.pdf?ref=blog.roboflow.com">GPT-4V(ision)</a> (GPT-4V) is a multimodal model developed by OpenAI. GPT-4V allows a user to upload an image as an input and ask a question about the image, a task type known as visual question answering (VQA).<br>
GPT-4Vï¼ˆè§†è§‰ç‰ˆï¼‰ï¼ˆGPT-4Vï¼‰æ˜¯ç”±OpenAIå¼€å‘çš„å¤šæ¨¡æ€æ¨¡å‹ã€‚GPT-4Vå…è®¸ç”¨æˆ·ä¸Šä¼ å›¾åƒä½œä¸ºè¾“å…¥ï¼Œå¹¶å¯¹å›¾åƒæå‡ºé—®é¢˜ï¼Œè¿™æ˜¯ä¸€ç§è¢«ç§°ä¸ºè§†è§‰é—®ç­”ï¼ˆVQAï¼‰çš„ä»»åŠ¡ç±»å‹ã€‚</p>
<p>GPT-4V is rolling out as of September 24th and will be available in both the OpenAI ChatGPT iOS app and the web interface. You must have a GPT-4 subscription to use the tool.<br>
ä»9æœˆ24æ—¥å¼€å§‹ï¼ŒGPT-4Vå·²ç»å¼€å§‹æ¨å‡ºï¼Œå¹¶å°†åœ¨OpenAI ChatGPT iOSåº”ç”¨ç¨‹åºå’ŒWebç•Œé¢ä¸Šæä¾›ã€‚æ‚¨å¿…é¡»æ‹¥æœ‰GPT-4è®¢é˜…æ‰èƒ½ä½¿ç”¨è¯¥å·¥å…·ã€‚</p>
<p>Letâ€™s experiment with GPT-4V and test its capabilities!<br>
è®©æˆ‘ä»¬æ¥å°è¯•ä¸€ä¸‹GPT-4Vå¹¶æµ‹è¯•å…¶èƒ½åŠ›ï¼</p>
<h2 id="test-1-visual-question-answering">Test #1: Visual Question Answering</h2>
<p>æµ‹è¯• #1: è§†è§‰é—®ç­”</p>
<p>One of our first experiments with GPT-4V was to inquire about a computer vision meme. We chose this experiment because it allows us to the extent to which GPT-4V understands context and relationships in a given image.<br>
æˆ‘ä»¬å¯¹GPT-4Vè¿›è¡Œçš„é¦–æ¬¡å®éªŒä¹‹ä¸€æ˜¯è¯¢é—®å…³äºè®¡ç®—æœºè§†è§‰è¿·å› çš„é—®é¢˜ã€‚æˆ‘ä»¬é€‰æ‹©äº†è¿™ä¸ªå®éªŒï¼Œå› ä¸ºå®ƒèƒ½è®©æˆ‘ä»¬äº†è§£GPT-4Våœ¨ç»™å®šå›¾åƒä¸­ç†è§£ä¸Šä¸‹æ–‡å’Œå…³ç³»çš„ç¨‹åº¦ã€‚</p>
<figure data-type="image" tabindex="2"><img src="https://blog.roboflow.com/content/images/2023/09/2023-09-26-17.25.07-1.jpg" alt="" loading="lazy"></figure>
<p>GPT-4V was able to successfully describe why the image was funny, making reference to various components of the image and how they connect. Notably, the provided meme contained text, which GPT-4V was able to read and use to generate a response. With that said, GPT-4V did make a mistake. The model said the fried chicken was labeled â€œNVIDIA BURGERâ€ instead of â€œGPUâ€.<br>
GPT-4VæˆåŠŸåœ°æè¿°äº†ä¸ºä»€ä¹ˆè¿™å¼ å›¾ç‰‡å¾ˆæœ‰è¶£ï¼Œå¼•ç”¨äº†å›¾ç‰‡çš„å„ä¸ªç»„æˆéƒ¨åˆ†ä»¥åŠå®ƒä»¬ä¹‹é—´çš„è”ç³»ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œæ‰€æä¾›çš„è¡¨æƒ…åŒ…åŒ…å«äº†æ–‡å­—ï¼ŒGPT-4Vèƒ½å¤Ÿé˜…è¯»å¹¶ç”¨æ¥ç”Ÿæˆå›åº”ã€‚è¯è™½å¦‚æ­¤ï¼ŒGPT-4Vä¹ŸçŠ¯äº†ä¸€ä¸ªé”™è¯¯ã€‚æ¨¡å‹è¯´ç‚¸é¸¡ä¸Šæ ‡ç€â€œNVIDIA BURGERâ€ï¼Œè€Œä¸æ˜¯â€œGPUâ€ã€‚</p>
<p>We then went on to test GPT-4V with currency, running a couple of different tests. First, we uploaded a photo of a United States penny. GPT-4V was able to successfully identify the origin and denomination of the coin:<br>
ç„¶åæˆ‘ä»¬ç»§ç»­ä½¿ç”¨è´§å¸å¯¹GPT-4Vè¿›è¡Œæµ‹è¯•ï¼Œè¿›è¡Œäº†å‡ ä¸ªä¸åŒçš„æµ‹è¯•ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬ä¸Šä¼ äº†ä¸€å¼ ç¾å›½ä¸€åˆ†ç¡¬å¸çš„ç…§ç‰‡ã€‚GPT-4VæˆåŠŸåœ°è¯†åˆ«å‡ºäº†ç¡¬å¸çš„æ¥æºå’Œé¢é¢ï¼š</p>
<figure data-type="image" tabindex="3"><img src="https://blog.roboflow.com/content/images/2023/09/Screenshot-2023-09-26-at-19.36.14-1.png" alt="" loading="lazy"></figure>
<p>We then uploaded an image with multiple coins and prompted GPT-4V with the text: â€œHow much money do I have?â€<br>
æˆ‘ä»¬éšåä¸Šä¼ äº†ä¸€å¼ æœ‰å¤šä¸ªç¡¬å¸çš„å›¾ç‰‡ï¼Œå¹¶ç”¨æ–‡æœ¬æç¤ºGPT-4Vï¼šâ€œæˆ‘æœ‰å¤šå°‘é’±ï¼Ÿâ€</p>
<figure data-type="image" tabindex="4"><img src="https://blog.roboflow.com/content/images/2023/09/2023-09-26-17.56.29.jpg" alt="" loading="lazy"></figure>
<p>GPT-4V was able to identify the number of coins but did not ascertain the currency type. With a follow up question, GPT-4V successfully identified the currency type:<br>
GPT-4Vèƒ½å¤Ÿè¯†åˆ«ç¡¬å¸çš„æ•°é‡ï¼Œä½†æ— æ³•ç¡®å®šè´§å¸ç±»å‹ã€‚é€šè¿‡åç»­çš„é—®é¢˜ï¼ŒGPT-4VæˆåŠŸåœ°ç¡®å®šäº†è´§å¸ç±»å‹ï¼š</p>
<figure data-type="image" tabindex="5"><img src="https://blog.roboflow.com/content/images/2023/09/2023-09-26-18.00.56.jpg" alt="" loading="lazy"></figure>
<p>Moving on to another topic, we decided to try using GPT-4V with a photo from a popular movie: Pulp Fiction. We wanted to know: could GPT-4 answer a question about the movie without being told in text what movie it was?<br>
è½¬åˆ°å¦ä¸€ä¸ªè¯é¢˜ï¼Œæˆ‘ä»¬å†³å®šå°è¯•ä½¿ç”¨GPT-4Vä¸ä¸€å¼ æ¥è‡ªä¸€éƒ¨çƒ­é—¨ç”µå½±ã€Šä½ä¿—å°è¯´ã€‹çš„ç…§ç‰‡ã€‚æˆ‘ä»¬æƒ³çŸ¥é“ï¼šGPT-4èƒ½å¦åœ¨æ²¡æœ‰ä»¥æ–‡æœ¬å½¢å¼å‘ŠçŸ¥ç”µå½±åç§°çš„æƒ…å†µä¸‹å›ç­”å…³äºè¯¥ç”µå½±çš„é—®é¢˜ï¼Ÿ</p>
<p>We uploaded a photo from Pulp Fiction with the prompt â€œIs it a good movie?â€, to which GPT-4V responded with a description of the movie and an answer to our question. GPT-4V provides a high-level description of the movie and a summary of the attributes associated with the movie considered to be positive and negative.<br>
æˆ‘ä»¬ä¸Šä¼ äº†ä¸€å¼ ã€Šä½ä¿—å°è¯´ã€‹çš„ç…§ç‰‡ï¼Œå¹¶é™„ä¸Šäº†â€œè¿™æ˜¯ä¸€éƒ¨å¥½ç”µå½±å—ï¼Ÿâ€çš„æç¤ºã€‚GPT-4Vå¯¹æ­¤ä½œå‡ºäº†å¯¹ç”µå½±çš„æè¿°ï¼Œå¹¶å›ç­”äº†æˆ‘ä»¬çš„é—®é¢˜ã€‚GPT-4Væä¾›äº†å¯¹ç”µå½±çš„é«˜å±‚æ¬¡æè¿°ï¼Œä»¥åŠå¯¹ç”µå½±æ‰€å…·æœ‰çš„æ­£é¢å’Œè´Ÿé¢å±æ€§çš„æ€»ç»“ã€‚</p>
<p>We further asked about the IMDB score for the movie, to which GPT-4V responded with the score as of January 2022. This suggests, like other GPT models released by OpenAI, there is a knowledge cutoff after which point the model has no more recent knowledge.<br>
æˆ‘ä»¬è¿›ä¸€æ­¥è¯¢é—®äº†å…³äºè¯¥ç”µå½±çš„IMDBè¯„åˆ†ï¼ŒGPT-4Vå›ç­”äº†2022å¹´1æœˆçš„è¯„åˆ†ã€‚è¿™è¡¨æ˜ï¼ŒåƒOpenAIå‘å¸ƒçš„å…¶ä»–GPTæ¨¡å‹ä¸€æ ·ï¼Œå­˜åœ¨ä¸€ä¸ªçŸ¥è¯†æˆªæ­¢ç‚¹ï¼Œè¶…è¿‡è¿™ä¸ªç‚¹åï¼Œæ¨¡å‹å°±æ²¡æœ‰æ›´å¤šçš„æœ€æ–°çŸ¥è¯†äº†ã€‚</p>
<figure data-type="image" tabindex="6"><img src="https://blog.roboflow.com/content/images/2023/09/2023-09-26-18.13.51.jpg" alt="" loading="lazy"></figure>
<p>We then explored GPT-4Vâ€™s question answering capabilities by asking a question about a place. We uploaded a photo of San Francisco with the text prompt â€œWhere is this?â€ GPT-4V successfully identified the location, San Francisco, and noted that the Transamerica Pyramid, pictured in the image we uploaded, is a notable landmark in the city.<br>
æˆ‘ä»¬éšåæ¢ç´¢äº†GPT-4Vçš„é—®ç­”èƒ½åŠ›ï¼Œé€šè¿‡è¯¢é—®ä¸€ä¸ªå…³äºåœ°ç‚¹çš„é—®é¢˜ã€‚æˆ‘ä»¬ä¸Šä¼ äº†ä¸€å¼ æ—§é‡‘å±±çš„ç…§ç‰‡ï¼Œå¹¶é™„ä¸Šäº†æ–‡å­—æç¤ºâ€œè¿™æ˜¯å“ªé‡Œï¼Ÿâ€GPT-4VæˆåŠŸåœ°è¯†åˆ«å‡ºäº†åœ°ç‚¹ï¼Œæ—§é‡‘å±±ï¼Œå¹¶æŒ‡å‡ºæˆ‘ä»¬ä¸Šä¼ çš„å›¾ç‰‡ä¸­çš„Transamerica Pyramidæ˜¯è¯¥åŸå¸‚çš„ä¸€ä¸ªè‘—ååœ°æ ‡ã€‚</p>
<figure data-type="image" tabindex="7"><img src="https://blog.roboflow.com/content/images/2023/09/Screenshot-2023-09-26-at-19.39.34.png" alt="" loading="lazy"></figure>
<p>Moving over to the realm of plants, we provided GPT-4V with a photo of a peace lily and asked the question â€œWhat is that plant and how should I care about it?â€:<br>
è½¬å‘æ¤ç‰©é¢†åŸŸï¼Œæˆ‘ä»¬å‘GPT-4Væä¾›äº†ä¸€å¼ å’Œå¹³ç™¾åˆçš„ç…§ç‰‡ï¼Œå¹¶é—®äº†è¿™æ ·ä¸€ä¸ªé—®é¢˜ï¼šâ€œé‚£æ˜¯ä»€ä¹ˆæ¤ç‰©ï¼Œæˆ‘åº”è¯¥å¦‚ä½•ç…§æ–™å®ƒï¼Ÿâ€</p>
<figure data-type="image" tabindex="8"><img src="https://blog.roboflow.com/content/images/2023/09/2023-09-27-13.06.19.jpg" alt="" loading="lazy"></figure>
<p>The model successfully identified that the plant is a peace lily and provided advice on how to care for the plant. This illustrates the utility of having text and vision combined to create a multi-modal such as they are in GPT-4V. The model returned a fluent answer to our question without having to build our own two-stage process (i.e. classification to identify the plant then GPT-4 to provide plant care advice).<br>
è¯¥æ¨¡å‹æˆåŠŸåœ°è¯†åˆ«å‡ºè¿™æ£µæ¤ç‰©æ˜¯ä¸€æ ªå’Œå¹³ç™¾åˆï¼Œå¹¶æä¾›äº†å¦‚ä½•ç…§æ–™è¿™æ£µæ¤ç‰©çš„å»ºè®®ã€‚è¿™è¯´æ˜äº†å°†æ–‡æœ¬å’Œè§†è§‰ç»“åˆèµ·æ¥åˆ›å»ºå¤šæ¨¡æ€çš„å®ç”¨æ€§ï¼Œå°±åƒåœ¨GPT-4Vä¸­ä¸€æ ·ã€‚è¯¥æ¨¡å‹å›ç­”äº†æˆ‘ä»¬çš„é—®é¢˜ï¼Œè€Œæ— éœ€æ„å»ºæˆ‘ä»¬è‡ªå·±çš„ä¸¤é˜¶æ®µè¿‡ç¨‹ï¼ˆå³é€šè¿‡åˆ†ç±»æ¥è¯†åˆ«æ¤ç‰©ï¼Œç„¶åä½¿ç”¨GPT-4æä¾›æ¤ç‰©æŠ¤ç†å»ºè®®ï¼‰ã€‚</p>
<h2 id="test-2-optical-character-recognition-ocr">Test #2: Optical Character Recognition (OCR)</h2>
<p>æµ‹è¯• #2ï¼šå…‰å­¦å­—ç¬¦è¯†åˆ«ï¼ˆOCRï¼‰</p>
<p>We conducted two tests to explore GPT-4Vâ€™s OCR capabilities: OCR on an image with text on a car tire and OCR on a photo of a paragraph from a digital document. Our intent was to build an understanding of how GPT-4V performs at OCR in the wild, where text may have less contrast and be at an angle, versus digital documents with clear text.<br>
æˆ‘ä»¬è¿›è¡Œäº†ä¸¤é¡¹æµ‹è¯•ï¼Œä»¥æ¢ç´¢GPT-4Vçš„OCRèƒ½åŠ›ï¼šå¯¹ä¸€å¼ å¸¦æœ‰æ±½è½¦è½®èƒä¸Šæ–‡å­—çš„å›¾ç‰‡è¿›è¡ŒOCRï¼Œä»¥åŠå¯¹ä¸€å¼ æ•°å­—æ–‡æ¡£æ®µè½çš„ç…§ç‰‡è¿›è¡ŒOCRã€‚æˆ‘ä»¬çš„ç›®çš„æ˜¯äº†è§£GPT-4Våœ¨é‡å¤–ç¯å¢ƒä¸­çš„OCRè¡¨ç°ï¼Œå…¶ä¸­æ–‡å­—å¯èƒ½å¯¹æ¯”åº¦è¾ƒä½ä¸”å‘ˆè§’åº¦ï¼Œä¸æ¸…æ™°æ–‡å­—çš„æ•°å­—æ–‡æ¡£ç›¸æ¯”ã€‚</p>
<figure data-type="image" tabindex="9"><img src="https://blog.roboflow.com/content/images/2023/09/2023-09-26-17.36.09-1.jpg" alt="" loading="lazy"></figure>
<p>GPT-4V was unable to correctly identify the serial number in an image of a tire. Some numbers were correct but there were several errors in the result from the model.<br>
GPT-4Væ— æ³•æ­£ç¡®è¯†åˆ«è½®èƒå›¾ç‰‡ä¸­çš„åºåˆ—å·ã€‚ä¸€äº›æ•°å­—æ˜¯æ­£ç¡®çš„ï¼Œä½†æ¨¡å‹çš„ç»“æœä¸­å­˜åœ¨å‡ ä¸ªé”™è¯¯ã€‚</p>
<p>In our document test, we presented text from a web page and asked GPT-4V to read the text in the image. The model was able to successfully identify the text in the image.<br>
åœ¨æˆ‘ä»¬çš„æ–‡æ¡£æµ‹è¯•ä¸­ï¼Œæˆ‘ä»¬å±•ç¤ºäº†æ¥è‡ªç½‘é¡µçš„æ–‡æœ¬ï¼Œå¹¶è¦æ±‚GPT-4Vé˜…è¯»å›¾åƒä¸­çš„æ–‡æœ¬ã€‚è¯¥æ¨¡å‹æˆåŠŸåœ°è¯†åˆ«å‡ºäº†å›¾åƒä¸­çš„æ–‡æœ¬ã€‚</p>
<figure data-type="image" tabindex="10"><img src="https://blog.roboflow.com/content/images/2023/09/File.jpg" alt="" loading="lazy"></figure>
<p>GPT-4V does an excellent job translating words in an image to individual characters in text. A useful insight for tasks related to extracting text from documents.<br>
GPT-4Våœ¨å°†å›¾åƒä¸­çš„å•è¯ç¿»è¯‘æˆæ–‡æœ¬ä¸­çš„ä¸ªåˆ«å­—ç¬¦æ–¹é¢è¡¨ç°å‡ºè‰²ã€‚è¿™å¯¹äºä»æ–‡æ¡£ä¸­æå–æ–‡æœ¬çš„ä»»åŠ¡éå¸¸æœ‰ç”¨çš„æ´å¯ŸåŠ›ã€‚</p>
<h2 id="test-3-math-ocr-æµ‹è¯•-3-æ•°å­¦-ocr">Test #3: Math OCR æµ‹è¯• #3: æ•°å­¦ OCR</h2>
<p>Math OCR is a specialized form of OCR pertaining specifically to math equations. Math OCR is often considered its own discipline because the syntax of what the OCR model needs to identify extends to a vast range of symbols.<br>
æ•°å­¦OCRæ˜¯ä¸€ç§ä¸“é—¨ç”¨äºæ•°å­¦æ–¹ç¨‹çš„OCRå½¢å¼ã€‚æ•°å­¦OCRé€šå¸¸è¢«è§†ä¸ºä¸€é—¨ç‹¬ç«‹çš„å­¦ç§‘ï¼Œå› ä¸ºOCRæ¨¡å‹éœ€è¦è¯†åˆ«çš„è¯­æ³•æ¶µç›–äº†å¹¿æ³›çš„ç¬¦å·èŒƒå›´ã€‚</p>
<p>We presented GPT-4V with a math question. This math question was in a screenshot taken from a document. The question concerns calculating the length of a zip wire given two angles. We presented the image with the prompt â€œSolve it.â€<br>
æˆ‘ä»¬å‘GPT-4Væå‡ºäº†ä¸€ä¸ªæ•°å­¦é—®é¢˜ã€‚è¿™ä¸ªæ•°å­¦é—®é¢˜æ˜¯ä»ä¸€ä¸ªæ–‡æ¡£çš„æˆªå›¾ä¸­å¾—åˆ°çš„ã€‚è¿™ä¸ªé—®é¢˜æ¶‰åŠè®¡ç®—ç»™å®šä¸¤ä¸ªè§’åº¦çš„æ»‘ç´¢é•¿åº¦ã€‚æˆ‘ä»¬ç”¨æç¤ºè¯­â€œè§£å†³å®ƒâ€å‘ˆç°äº†è¿™ä¸ªå›¾ç‰‡ã€‚</p>
<figure data-type="image" tabindex="11"><img src="https://blog.roboflow.com/content/images/2023/09/2023-09-27-13.25.51.jpg" alt="" loading="lazy"></figure>
<figure data-type="image" tabindex="12"><img src="https://blog.roboflow.com/content/images/2023/09/photo_2023-09-27-13.25.55.jpeg" alt="" loading="lazy"></figure>
<p>The model identified the problem can be solved with trigonometry, identified the function to use, and presented a step-by-step walkthrough of how to solve the problem. Then, GPT-4V provided the correct answer to the question.<br>
æ¨¡å‹ç¡®å®šäº†å¯ä»¥ç”¨ä¸‰è§’å­¦è§£å†³çš„é—®é¢˜ï¼Œå¹¶ç¡®å®šäº†è¦ä½¿ç”¨çš„å‡½æ•°ï¼Œå¹¶æä¾›äº†è§£å†³é—®é¢˜çš„é€æ­¥æŒ‡å¯¼ã€‚ç„¶åï¼ŒGPT-4Væä¾›äº†æ­£ç¡®çš„ç­”æ¡ˆã€‚</p>
<p>With that said, the GPT-4V system card notes that the model may miss mathematical symbols. Different tests, including tests where an equation or expression is written by hand on paper, may indicate deficiencies in the model's ability to answer math questions.<br>
è¯è™½å¦‚æ­¤ï¼ŒGPT-4Vç³»ç»Ÿå¡ç‰‡æŒ‡å‡ºè¯¥æ¨¡å‹å¯èƒ½ä¼šæ¼æ‰æ•°å­¦ç¬¦å·ã€‚ä¸åŒçš„æµ‹è¯•ï¼ŒåŒ…æ‹¬æ‰‹å†™åœ¨çº¸ä¸Šçš„æ–¹ç¨‹æˆ–è¡¨è¾¾å¼çš„æµ‹è¯•ï¼Œå¯èƒ½ä¼šæ˜¾ç¤ºå‡ºè¯¥æ¨¡å‹å›ç­”æ•°å­¦é—®é¢˜çš„èƒ½åŠ›å­˜åœ¨ä¸è¶³ã€‚</p>
<h2 id="test-4-object-detection">Test #4: Object Detection</h2>
<p>æµ‹è¯• #4: ç›®æ ‡æ£€æµ‹</p>
<p><a href="https://blog.roboflow.com/object-detection/">Object detection</a> is a fundamental task in the field of computer vision. We asked GPT-4V to identify the location of various objects to evaluate its ability to perform object detection tasks.<br>
ç›®æ ‡æ£€æµ‹æ˜¯è®¡ç®—æœºè§†è§‰é¢†åŸŸä¸­çš„åŸºæœ¬ä»»åŠ¡ã€‚æˆ‘ä»¬è¦æ±‚GPT-4Vè¯†åˆ«å„ç§ç‰©ä½“çš„ä½ç½®ï¼Œä»¥è¯„ä¼°å…¶è¿›è¡Œç›®æ ‡æ£€æµ‹ä»»åŠ¡çš„èƒ½åŠ›ã€‚</p>
<p>In our first test, we asked GPT-4V to detect a dog in an image and provide the x_min, y_min, x_max, and y_max values associated with the position of the dog. The bounding box coordinates returned by GPT-4V did not match the position of the dog.<br>
åœ¨æˆ‘ä»¬çš„ç¬¬ä¸€æ¬¡æµ‹è¯•ä¸­ï¼Œæˆ‘ä»¬è¦æ±‚GPT-4Våœ¨å›¾åƒä¸­æ£€æµ‹ç‹—ï¼Œå¹¶æä¾›ä¸ç‹—çš„ä½ç½®ç›¸å…³çš„x_minã€y_minã€x_maxå’Œy_maxå€¼ã€‚ç”±GPT-4Vè¿”å›çš„è¾¹ç•Œæ¡†åæ ‡ä¸ç‹—çš„ä½ç½®ä¸åŒ¹é…ã€‚</p>
<figure data-type="image" tabindex="13"><img src="https://blog.roboflow.com/content/images/2023/09/photo_2023-09-26-18.51.24.jpeg" alt="" loading="lazy"></figure>
<p>While GPT-4Vâ€™s capabilities at answering questions about an image are powerful, the model is not a substitute for fine-tuned <a href="https://roboflow.com/models/object-detection?ref=blog.roboflow.com">object detection models</a> in scenarios where you want to know where an object is in an image.<br>
è™½ç„¶GPT-4Våœ¨å›ç­”å…³äºå›¾åƒçš„é—®é¢˜æ–¹é¢å…·æœ‰å¼ºå¤§çš„èƒ½åŠ›ï¼Œä½†åœ¨æƒ³è¦çŸ¥é“å›¾åƒä¸­ç‰©ä½“ä½ç½®çš„åœºæ™¯ä¸­ï¼Œè¯¥æ¨¡å‹å¹¶ä¸èƒ½æ›¿ä»£ç»è¿‡ç²¾ç»†è°ƒæ•´çš„ç‰©ä½“æ£€æµ‹æ¨¡å‹ã€‚</p>
<h2 id="test-5-captcha-æµ‹è¯•-5-éªŒè¯ç ">Test #5: CAPTCHA æµ‹è¯• #5: éªŒè¯ç </h2>
<p>We decided to test GPT-4V with CAPTCHAs, a task OpenAI studied in their research and wrote about in their <a href="https://cdn.openai.com/papers/GPTV_System_Card.pdf?ref=blog.roboflow.com">system card</a>. We found that GPT-4V was able to identify that an image contained a CAPTCHA but often failed the tests. In a traffic light example, GPT-4V missed some boxes that contained traffic lights.<br>
æˆ‘ä»¬å†³å®šç”¨CAPTCHAæµ‹è¯•GPT-4Vï¼Œè¿™æ˜¯OpenAIåœ¨ä»–ä»¬çš„ç ”ç©¶ä¸­ç ”ç©¶å¹¶åœ¨ä»–ä»¬çš„ç³»ç»Ÿå¡ç‰‡ä¸­å†™åˆ°çš„ä¸€ä¸ªä»»åŠ¡ã€‚æˆ‘ä»¬å‘ç°GPT-4Vèƒ½å¤Ÿè¯†åˆ«å‡ºå›¾åƒä¸­åŒ…å«äº†CAPTCHAï¼Œä½†åœ¨æµ‹è¯•ä¸­ç»å¸¸å¤±è´¥ã€‚åœ¨ä¸€ä¸ªäº¤é€šç¯çš„ä¾‹å­ä¸­ï¼ŒGPT-4Vé”™è¿‡äº†ä¸€äº›åŒ…å«äº¤é€šç¯çš„æ–¹æ¡†ã€‚</p>
<figure data-type="image" tabindex="14"><img src="https://blog.roboflow.com/content/images/2023/09/photo_2023-09-27-13.01.22.jpeg" alt="" loading="lazy"></figure>
<p>In the following crosswalk example, GPT-4V classified a few boxes correctly but incorrectly classified one box in the CAPTCHA as a crosswalk.<br>
åœ¨ä¸‹é¢çš„äººè¡Œæ¨ªé“ç¤ºä¾‹ä¸­ï¼ŒGPT-4Væ­£ç¡®åˆ†ç±»äº†å‡ ä¸ªæ–¹æ¡†ï¼Œä½†é”™è¯¯åœ°å°†ä¸€ä¸ªCAPTCHAæ–¹æ¡†åˆ†ç±»ä¸ºäººè¡Œæ¨ªé“ã€‚</p>
<figure data-type="image" tabindex="15"><img src="https://lh4.googleusercontent.com/sUn71XmNZHeS4C9U1KGZm9T12MPiDaWSnjeqqZXSTan3I01VVBMvJ0_8knDTQW6kO1YJS8jLXswk_zEyINNDQz7mwDT60e_NoKrikqwaKuULsM9upmURmKCZ7STF6INGj4FtvEY3jlIjvgpVi1eamCI" alt="" loading="lazy"></figure>
<h2 id="test-6-crosswords-and-sudokus">Test #6: Crosswords and Sudoku's</h2>
<p>æµ‹è¯• #6ï¼šå¡«å­—æ¸¸æˆå’Œæ•°ç‹¬</p>
<p>We decided to test how GPT-4V performs on crosswords and sudokus.<br>
æˆ‘ä»¬å†³å®šæµ‹è¯•GPT-4Våœ¨å¡«å­—æ¸¸æˆå’Œæ•°ç‹¬ä¸Šçš„è¡¨ç°ã€‚</p>
<p>First, we prompted GPT-4V with photos of a crossword with the text instruction &quot;Solve it.&quot; GPT-4V inferred the image contained a crossword and attempted to provide a solution to the crossword. The model appeared to read the clues correctly but misinterpreted the structure of the board. As a result, the provided answers were incorrect.<br>
é¦–å…ˆï¼Œæˆ‘ä»¬ç”¨ä¸€å¼ å¸¦æœ‰æ–‡å­—æŒ‡ä»¤â€œè§£ç­”å®ƒâ€çš„çºµæ¨ªå­—è°œç…§ç‰‡æ¥å¯åŠ¨GPT-4Vã€‚GPT-4Væ¨æµ‹è¿™å¼ å›¾ç‰‡åŒ…å«äº†ä¸€ä¸ªçºµæ¨ªå­—è°œï¼Œå¹¶è¯•å›¾æä¾›ä¸€ä¸ªè§£ç­”ã€‚æ¨¡å‹ä¼¼ä¹èƒ½æ­£ç¡®é˜…è¯»æç¤ºï¼Œä½†å¯¹äºå­—è°œæ¿çš„ç»“æ„ç†è§£æœ‰è¯¯ã€‚å› æ­¤ï¼Œæä¾›çš„ç­”æ¡ˆæ˜¯é”™è¯¯çš„ã€‚</p>
<figure data-type="image" tabindex="16"><img src="https://lh6.googleusercontent.com/bXAg1SiRBcs-huLBicWFzkeKI8NxB5OE1zoa1cAvC8sqfU1aFmZ2MRDKd2PTKxafivJsaY3R189vJYPEx0BzrXyWwy5ta2TEaGU2yKrBrOxqCYiQhAM93N4SDvZu6Wb7S3lCGaB2j9PxUCvuqbWD8os" alt="" loading="lazy"></figure>
<p>This same limitation was exhibited in our sudoku test, where GPT-4V identified the game but misunderstood the structure of the board and thus returned inaccurate results:<br>
åœ¨æˆ‘ä»¬çš„æ•°ç‹¬æµ‹è¯•ä¸­ï¼ŒGPT-4Vä¹Ÿå±•ç¤ºå‡ºäº†åŒæ ·çš„é™åˆ¶ï¼Œå®ƒèƒ½å¤Ÿè¯†åˆ«å‡ºæ¸¸æˆï¼Œä½†å¯¹äºæ•°ç‹¬æ£‹ç›˜çš„ç»“æ„å´å­˜åœ¨è¯¯è§£ï¼Œå› æ­¤è¿”å›äº†ä¸å‡†ç¡®çš„ç»“æœ</p>
<figure data-type="image" tabindex="17"><img src="https://lh4.googleusercontent.com/U9cH5wYei3jZN8mmAA6etp3ngH8Zu0YrpLisXW6CEO0uSDB-FW3UO7PDLm-u5sEwc6Isvvh3BP_qizYEZctgWRUQpt8oP2_ius6vKGvUmTmAdcn6eneWiAOgq1O6n2W1LV7rx6a6hmDXLxrHs7IkxZI" alt="" loading="lazy"></figure>
<h2 id="gpt-4v-limitations-and-safety">GPT-4V Limitations and Safety</h2>
<p>GPT-4Vçš„é™åˆ¶å’Œå®‰å…¨æ€§</p>
<p>OpenAI conducted research with an alpha version of the vision model available to a small group of users, as outlined in the official <a href="https://cdn.openai.com/papers/GPTV_System_Card.pdf?ref=blog.roboflow.com">GPT-4V(ision) System Card</a>. During this process, they were able to gather feedback and insights on how GPT-4V works with prompts provided by a range of people. This was supplemented with â€œred teamingâ€, wherein external experts were â€œto qualitatively assess the limitations and risks associated with the model and systemâ€.<br>
OpenAIåœ¨ä¸€å°éƒ¨åˆ†ç”¨æˆ·ä¸­ä½¿ç”¨GPT-4V(ision)ç³»ç»Ÿå¡çš„Î±ç‰ˆæœ¬è¿›è¡Œäº†ç ”ç©¶ã€‚åœ¨è¿™ä¸ªè¿‡ç¨‹ä¸­ï¼Œä»–ä»¬èƒ½å¤Ÿæ”¶é›†åˆ°ç”¨æˆ·å¯¹GPT-4Vçš„åé¦ˆå’Œè§è§£ï¼Œè¿™äº›åé¦ˆæ˜¯é€šè¿‡å„ç§äººæä¾›çš„æç¤ºå¾—åˆ°çš„ã€‚æ­¤å¤–ï¼Œè¿˜è¿›è¡Œäº†â€œçº¢é˜Ÿè¯„ä¼°â€ï¼Œå³ç”±å¤–éƒ¨ä¸“å®¶â€œå®šæ€§è¯„ä¼°æ¨¡å‹å’Œç³»ç»Ÿçš„é™åˆ¶å’Œé£é™©â€ã€‚</p>
<p>Based on OpenAIâ€™s research, the GPT-4V system card notes numerous limitations with the model such as:<br>
æ ¹æ®OpenAIçš„ç ”ç©¶ï¼ŒGPT-4Vç³»ç»Ÿå¡ä¸­æŒ‡å‡ºäº†è¯¥æ¨¡å‹çš„è®¸å¤šé™åˆ¶ï¼Œä¾‹å¦‚ï¼š</p>
<ol>
<li>Missing text or characters in an image<br>
å›¾åƒä¸­ç¼ºå°‘æ–‡æœ¬æˆ–å­—ç¬¦</li>
<li>Missing mathematical symbols<br>
ç¼ºå°‘æ•°å­¦ç¬¦å·</li>
<li>Being unable to recognize spatial locations and colors<br>
æ— æ³•è¯†åˆ«ç©ºé—´ä½ç½®å’Œé¢œè‰²</li>
</ol>
<p>In addition to limitations, OpenAI identified, researched, and attempted to mitigate several risks associated with the model. For example, GPT-4V avoids identifying a specific person in an image and does not respond to prompts pertaining to hate symbols.<br>
é™¤äº†é™åˆ¶ä¹‹å¤–ï¼ŒOpenAIè¿˜ç¡®å®šã€ç ”ç©¶å¹¶è¯•å›¾å‡è½»ä¸è¯¥æ¨¡å‹ç›¸å…³çš„å‡ ä¸ªé£é™©ã€‚ä¾‹å¦‚ï¼ŒGPT-4Vé¿å…åœ¨å›¾åƒä¸­è¯†åˆ«ç‰¹å®šäººç‰©ï¼Œå¹¶ä¸”ä¸ä¼šå›åº”ä¸ä»‡æ¨ç¬¦å·ç›¸å…³çš„æç¤ºã€‚</p>
<p>With that said, there is further work to be done in model safeguarding. For example, OpenAI notes in the model system card that â€œIf prompted, GPT-4V can generate content praising certain lesser known hate groups in response to their symbols.â€,<br>
è¯è™½å¦‚æ­¤ï¼Œæ¨¡å‹ä¿æŠ¤æ–¹é¢è¿˜æœ‰è¿›ä¸€æ­¥çš„å·¥ä½œè¦åšã€‚ä¾‹å¦‚ï¼ŒOpenAIåœ¨æ¨¡å‹ç³»ç»Ÿå¡ä¸­æŒ‡å‡ºï¼šâ€œå¦‚æœæœ‰æç¤ºï¼ŒGPT-4Vå¯èƒ½ä¼šç”Ÿæˆèµæ‰¬æŸäº›è¾ƒä¸ºä¸çŸ¥åçš„ä»‡æ¨å›¢ä½“çš„å†…å®¹ï¼Œä»¥å›åº”ä»–ä»¬çš„ç¬¦å·ã€‚â€</p>
<h2 id="gpt-4v-for-computer-vision-and-beyond">GPT-4V for Computer Vision and Beyond</h2>
<p>GPT-4V ç”¨äºè®¡ç®—æœºè§†è§‰åŠæ›´å¤šé¢†åŸŸ</p>
<p>GPT-4V is a notable movement in the field of machine learning and natural language processing. With GPT-4V, you can ask questions about an image â€“ and follow up questions â€“ in natural language and the model will attempt to ask your question.<br>
GPT-4Væ˜¯æœºå™¨å­¦ä¹ å’Œè‡ªç„¶è¯­è¨€å¤„ç†é¢†åŸŸçš„ä¸€é¡¹é‡è¦è¿›å±•ã€‚é€šè¿‡GPT-4Vï¼Œæ‚¨å¯ä»¥ç”¨è‡ªç„¶è¯­è¨€æå‡ºå…³äºå›¾åƒçš„é—®é¢˜ï¼Œä»¥åŠåç»­çš„é—®é¢˜ï¼Œæ¨¡å‹å°†å°è¯•å›ç­”æ‚¨çš„é—®é¢˜ã€‚</p>
<p>GPT-4V performed well at various general image questions and demonstrated awareness of context in some images we tested. For instance, GPT-4V was able to successfully answer questions about a movie featured in an image without being told in text what the movie was.ğŸ”„Â Â â“</p>
<p>For general question answering, GPT-4V is exciting. While models existed for this purpose in the past, they often lacked fluency in their answers. GPT-4V is able to both answer questions and follow up questions about an image and do so in depth.<br>
å¯¹äºä¸€èˆ¬çš„é—®é¢˜å›ç­”ï¼ŒGPT-4Véå¸¸ä»¤äººå…´å¥‹ã€‚è™½ç„¶è¿‡å»å·²ç»å­˜åœ¨ç”¨äºæ­¤ç›®çš„çš„æ¨¡å‹ï¼Œä½†å®ƒä»¬åœ¨å›ç­”é—®é¢˜æ—¶å¸¸å¸¸ç¼ºä¹æµç•…æ€§ã€‚GPT-4Vèƒ½å¤Ÿå›ç­”å…³äºå›¾åƒçš„é—®é¢˜ä»¥åŠåç»­é—®é¢˜ï¼Œå¹¶ä¸”èƒ½å¤Ÿæ·±å…¥åœ°è¿›è¡Œå›ç­”ã€‚</p>
<p>With GPT-4V, you can ask questions about an image without creating a two-stage process (i.e. classification then using the results to ask a question to a language model like GPT). There will likely be limitations to what GPT-4V can understand, hence testing a use case to understand how the model performs is crucial.<br>
é€šè¿‡GPT-4Vï¼Œæ‚¨å¯ä»¥åœ¨ä¸åˆ›å»ºä¸¤ä¸ªé˜¶æ®µçš„è¿‡ç¨‹ï¼ˆå³å…ˆåˆ†ç±»ï¼Œç„¶åä½¿ç”¨ç»“æœå‘GPTç­‰è¯­è¨€æ¨¡å‹æé—®ï¼‰çš„æƒ…å†µä¸‹ï¼Œå¯¹å›¾åƒæå‡ºé—®é¢˜ã€‚GPT-4Vå¯èƒ½ä¼šæœ‰ä¸€äº›ç†è§£èƒ½åŠ›çš„é™åˆ¶ï¼Œå› æ­¤æµ‹è¯•ä½¿ç”¨æ¡ˆä¾‹ä»¥äº†è§£æ¨¡å‹çš„è¡¨ç°è‡³å…³é‡è¦ã€‚</p>
<p>With that said, GPT-4V has its limitations. The model did â€œhallucinateâ€, wherein the model returned inaccurate information. This is a risk with using language models to answer questions. Furthermore, the model was unable to accurately return bounding boxes for object detection, suggesting it is unfit for this use case currently.<br>
è¯è™½å¦‚æ­¤ï¼ŒGPT-4Vä¹Ÿæœ‰å…¶å±€é™æ€§ã€‚è¯¥æ¨¡å‹ä¼šå‡ºç°â€œå¹»è§‰â€ï¼Œè¿”å›ä¸å‡†ç¡®çš„ä¿¡æ¯ã€‚è¿™æ˜¯ä½¿ç”¨è¯­è¨€æ¨¡å‹å›ç­”é—®é¢˜æ—¶çš„é£é™©ã€‚æ­¤å¤–ï¼Œè¯¥æ¨¡å‹æ— æ³•å‡†ç¡®è¿”å›ç‰©ä½“æ£€æµ‹çš„è¾¹ç•Œæ¡†ï¼Œæš—ç¤ºç›®å‰ä¸é€‚åˆè¿™ç§ç”¨ä¾‹ã€‚</p>
<p>We also observed that GPT-4V is unable to answer questions about people. When given a photo of Taylor Swift and asked who was featured in the image, the model declined to answer. OpenAI define this as an expected behavior in the published system card.<br>
æˆ‘ä»¬è¿˜è§‚å¯Ÿåˆ°ï¼ŒGPT-4Væ— æ³•å›ç­”å…³äºäººçš„é—®é¢˜ã€‚å½“ç»™å‡ºä¸€å¼ æ³°å‹’Â·æ–¯å¨å¤«ç‰¹çš„ç…§ç‰‡ï¼Œå¹¶è¯¢é—®å›¾åƒä¸­çš„äººæ˜¯è°æ—¶ï¼Œæ¨¡å‹æ‹’ç»å›ç­”ã€‚OpenAIåœ¨å‘å¸ƒçš„ç³»ç»Ÿå¡ä¸­å°†æ­¤å®šä¹‰ä¸ºé¢„æœŸè¡Œä¸ºã€‚</p>

              </div>
              <div class="toc-container">
                <ul class="markdownIt-TOC">
<li>
<ul>
<li><a href="#what-is-gpt-4v-gpt-4v%E6%98%AF%E4%BB%80%E4%B9%88">What is GPT-4V? GPT-4Væ˜¯ä»€ä¹ˆï¼Ÿ</a></li>
<li><a href="#test-1-visual-question-answering">Test #1: Visual Question Answering</a></li>
<li><a href="#test-2-optical-character-recognition-ocr">Test #2: Optical Character Recognition (OCR)</a></li>
<li><a href="#test-3-math-ocr-%E6%B5%8B%E8%AF%95-3-%E6%95%B0%E5%AD%A6-ocr">Test #3: Math OCR æµ‹è¯• #3: æ•°å­¦ OCR</a></li>
<li><a href="#test-4-object-detection">Test #4: Object Detection</a></li>
<li><a href="#test-5-captcha-%E6%B5%8B%E8%AF%95-5-%E9%AA%8C%E8%AF%81%E7%A0%81">Test #5: CAPTCHA æµ‹è¯• #5: éªŒè¯ç </a></li>
<li><a href="#test-6-crosswords-and-sudokus">Test #6: Crosswords and Sudoku's</a></li>
<li><a href="#gpt-4v-limitations-and-safety">GPT-4V Limitations and Safety</a></li>
<li><a href="#gpt-4v-for-computer-vision-and-beyond">GPT-4V for Computer Vision and Beyond</a></li>
</ul>
</li>
</ul>

              </div>
            </div>
          </article>
        </div>

        
          <div class="next-post">
            <div class="next">ä¸‹ä¸€ç¯‡</div>
            <a href="https://temberature.github.io/post/hAqkCRo01/">
              <h3 class="post-title">
                ä¸ºå…‹åŠ³å¾·çš„é•¿ä¸Šä¸‹æ–‡çª—å£è¿›è¡Œå¿«é€Ÿå·¥ç¨‹åŒ– Prompt engineering for Claude&#39;s long context window  
              </h3>
            </a>
          </div>
        

        
          

          
            <link rel="stylesheet" href="https://unpkg.com/disqusjs@1.1/dist/disqusjs.css">
<script src="https://unpkg.com/disqusjs@1.1/dist/disqus.js"></script>

<div id="disqus_thread"></div>

<script>

var options = {
  shortname: 'https-blog-talkgpt-space',
  apikey: 'DIRSOBifr8Ilq70AtPOLKvL83R1JlYaEhRFKlxlDh1eb9VWpF9zxmvhH1r8T6YWH',
}
if ('') {
  options.api = ''
}
var dsqjs = new DisqusJS(options)

</script>

          
        

        <div class="site-footer">
  Powered by <a href="https://github.com/getgridea/gridea" target="_blank">Gridea</a>
  <a class="rss" href="https://temberature.github.io/atom.xml" target="_blank">
    <i class="ri-rss-line"></i> RSS
  </a>
</div>

      </div>
    </div>

    <script>
      hljs.initHighlightingOnLoad()

      let mainNavLinks = document.querySelectorAll(".markdownIt-TOC a");

      // This should probably be throttled.
      // Especially because it triggers during smooth scrolling.
      // https://lodash.com/docs/4.17.10#throttle
      // You could do like...
      // window.addEventListener("scroll", () => {
      //    _.throttle(doThatStuff, 100);
      // });
      // Only not doing it here to keep this Pen dependency-free.

      window.addEventListener("scroll", event => {
        let fromTop = window.scrollY;

        mainNavLinks.forEach((link, index) => {
          let section = document.getElementById(decodeURI(link.hash).substring(1));
          let nextSection = null
          if (mainNavLinks[index + 1]) {
            nextSection = document.getElementById(decodeURI(mainNavLinks[index + 1].hash).substring(1));
          }
          if (section.offsetTop <= fromTop) {
            if (nextSection) {
              if (nextSection.offsetTop > fromTop) {
                link.classList.add("current");
              } else {
                link.classList.remove("current");    
              }
            } else {
              link.classList.add("current");
            }
          } else {
            link.classList.remove("current");
          }
        });
      });

    </script>
  </body>
</html>
